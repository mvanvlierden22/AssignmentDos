{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a29a05-f329-4dd7-af60-497965ef80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|flightDirection|count|\n",
      "+---------------+-----+\n",
      "|              D| 2469|\n",
      "|              A| 2451|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDirectionAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "    flight_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(\"/home/jovyan/data/2023-10-02.csv\")\n",
    "\n",
    "    # Count the number of arrival and departure flights\n",
    "    flight_counts = flight_data.groupBy(\"flightDirection\").count()\n",
    "\n",
    "    # Show the results\n",
    "    flight_counts.show()\n",
    "\n",
    "    # Optionally, you can save the results to a file or another data source\n",
    "    # flight_counts.write.format(\"parquet\").mode(\"overwrite\").save(\"path/to/save/results\")\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9e9948-a29f-411a-814c-21f63f2dff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average landing time offset: -0.10 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LandingTimeAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "    flight_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(\"/home/jovyan/data/2023-10-02.csv\")\n",
    "\n",
    "    # Convert timestamp columns to Spark TimestampType\n",
    "    flight_data = flight_data.withColumn(\"actualLandingTime\", col(\"actualLandingTime\").cast(\"timestamp\"))\n",
    "    flight_data = flight_data.withColumn(\"estimatedLandingTime\", col(\"estimatedLandingTime\").cast(\"timestamp\"))\n",
    "\n",
    "    # Calculate the offset between estimated and actual landing times\n",
    "    landing_time_offset = flight_data \\\n",
    "        .withColumn(\"landingTimeOffset\", (col(\"actualLandingTime\").cast(\"long\") - col(\"estimatedLandingTime\").cast(\"long\")) / 60)  # Convert seconds to minutes\n",
    "\n",
    "    # Calculate the average offset\n",
    "    average_offset = landing_time_offset.agg({\"landingTimeOffset\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "    # Show the results\n",
    "    print(f\"Average landing time offset: {average_offset:.2f} minutes\")\n",
    "\n",
    "    # Optionally, you can save the results to a file or another data source\n",
    "    # landing_time_offset.write.format(\"parquet\").mode(\"overwrite\").save(\"path/to/save/results\")\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0440e788-d60f-4b0f-8885-62070f46423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departure Gate Utilization:\n",
      "+----+-------------------+\n",
      "|gate|gatePercentage     |\n",
      "+----+-------------------+\n",
      "|D81 |0.8910490076954233 |\n",
      "|C6  |0.9315512353179425 |\n",
      "|B34 |1.6200891049007695 |\n",
      "|C22 |0.16200891049007696|\n",
      "|D28 |0.08100445524503848|\n",
      "|H6  |0.12150668286755771|\n",
      "|D16 |0.24301336573511542|\n",
      "|D7  |0.688537869582827  |\n",
      "|D5  |0.12150668286755771|\n",
      "|B22 |1.0530579181855002 |\n",
      "|B30 |2.67314702308627   |\n",
      "|D29 |0.850546780072904  |\n",
      "|D87 |1.0530579181855002 |\n",
      "|M4  |0.24301336573511542|\n",
      "|D27 |0.7695423248278656 |\n",
      "|D53 |0.16200891049007696|\n",
      "|D73 |0.6075334143377886 |\n",
      "|B4  |1.7820980153908466 |\n",
      "|M7  |0.04050222762251924|\n",
      "|D18 |0.28351559335763465|\n",
      "+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Departure Pier Utilization:\n",
      "+-------+------------------+\n",
      "|pier   |pierPercentage    |\n",
      "+-------+------------------+\n",
      "|F      |3.1186715269339813|\n",
      "|E      |5.30579181855002  |\n",
      "|B      |31.632239773187525|\n",
      "|M      |1.2960712839206157|\n",
      "|Unknown|2.187120291616039 |\n",
      "|D      |36.85702713649251 |\n",
      "|C      |16.362899959497774|\n",
      "|G      |2.1466180639935195|\n",
      "|H      |1.0935601458080195|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DepartureGatePierUtilizationAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "    flight_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(\"/home/jovyan/data/2023-10-02.csv\")\n",
    "\n",
    "    # Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "    flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "    # Filter only departure flights\n",
    "    departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "    # Group by gate and pier, and count the number of departure flights for each\n",
    "    gate_counts = departure_data.groupBy(\"gate\").count().withColumnRenamed(\"count\", \"gateCount\")\n",
    "    pier_counts = departure_data.groupBy(\"pier\").count().withColumnRenamed(\"count\", \"pierCount\")\n",
    "\n",
    "    # Calculate the total number of non-null departure flights\n",
    "    total_departure_flights = departure_data.na.drop(subset=[\"gate\", \"pier\"]).count()\n",
    "\n",
    "    # Calculate the percentage of gate utilization for departures\n",
    "    gate_utilization = gate_counts.withColumn(\"gatePercentage\", (col(\"gateCount\") / total_departure_flights) * 100)\n",
    "\n",
    "    # Calculate the percentage of pier utilization for departures\n",
    "    pier_utilization = pier_counts.withColumn(\"pierPercentage\", (col(\"pierCount\") / total_departure_flights) * 100)\n",
    "\n",
    "    # Show the results\n",
    "    print(\"Departure Gate Utilization:\")\n",
    "    gate_utilization.select(\"gate\", \"gatePercentage\").show(truncate=False)\n",
    "\n",
    "    print(\"Departure Pier Utilization:\")\n",
    "    pier_utilization.select(\"pier\", \"pierPercentage\").show(truncate=False)\n",
    "\n",
    "    # Optionally, you can save the results to a file or another data source\n",
    "    # gate_utilization.write.format(\"parquet\").mode(\"overwrite\").save(\"path/to/save/gate_results\")\n",
    "    # pier_utilization.write.format(\"parquet\").mode(\"overwrite\").save(\"path/to/save/pier_results\")\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25afc0c-9552-4313-8cb6-7b7cc3b26723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Type Utilization for Departures:\n",
      "+-----------+-------------------+\n",
      "|serviceType|percentage         |\n",
      "+-----------+-------------------+\n",
      "|F          |0.9315512353179425 |\n",
      "|C          |0.3645200486026731 |\n",
      "|J          |98.58242203321183  |\n",
      "|P          |0.12150668286755771|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ServiceTypePercentageAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "    flight_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(\"/home/jovyan/data/2023-10-02.csv\")\n",
    "\n",
    "    # Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "    flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "    # Filter only departure flights\n",
    "    departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "    # Group by service type and count the number of departure flights for each\n",
    "    service_type_counts = departure_data.groupBy(\"serviceType\").count().withColumnRenamed(\"count\", \"serviceTypeCount\")\n",
    "\n",
    "    # Calculate the total number of departure flights\n",
    "    total_departure_flights = departure_data.count()\n",
    "\n",
    "    # Calculate the percentage of service type utilization for departures\n",
    "    service_type_percentage = service_type_counts.withColumn(\"percentage\", (col(\"serviceTypeCount\") / total_departure_flights) * 100)\n",
    "\n",
    "    # Show the results\n",
    "    print(\"Service Type Utilization for Departures:\")\n",
    "    service_type_percentage.select(\"serviceType\", \"percentage\").show(truncate=False)\n",
    "\n",
    "    # Optionally, you can save the results to a file or another data source\n",
    "    # service_type_percentage.write.format(\"parquet\").mode(\"overwrite\").save(\"path/to/save/service_type_results\")\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72050fc5-3f0b-4ecf-97e3-1fd637a13974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[destination: string, flightCount: bigint]\n",
      "Top 10 Destinations for Departures:\n",
      "+-----------+-----------+\n",
      "|destination|flightCount|\n",
      "+-----------+-----------+\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, from_json, split\n",
    "from pyspark.sql.types import StringType, MapType, ArrayType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TopDestinationsAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "    flight_data = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(\"/home/jovyan/data/2023-10-02.csv\")\n",
    "\n",
    "    # Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "    flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "    # Filter only departure flights\n",
    "    departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "    # Convert the string representation of the route to a dictionary\n",
    "    departure_data = departure_data.withColumn(\"route_dict\", from_json(col(\"route\"), MapType(StringType(), StringType())))\n",
    "\n",
    "    # Split the destinations string into an array\n",
    "    departure_data = departure_data.withColumn(\"destinations_array\", split(col(\"route_dict.destinations\"), \",\"))\n",
    "\n",
    "    # Explode the destinations array to have one row per destination\n",
    "    destinations_data = departure_data.select(\"destinations_array\").withColumn(\"destination\", explode(\"destinations_array\"))\n",
    "\n",
    "    # Group by destination and count the number of departure flights for each\n",
    "    destination_counts = destinations_data.groupBy(\"destination\").count().withColumnRenamed(\"count\", \"flightCount\")\n",
    "    print(destination_counts)\n",
    "    \n",
    "    # Sort the destinations by flight count in descending order\n",
    "    sorted_destinations = destination_counts.orderBy(col(\"flightCount\").desc())\n",
    "\n",
    "    # Take the top 10 destinations\n",
    "    top_10_destinations = sorted_destinations.limit(10)\n",
    "\n",
    "    # Show the results\n",
    "    print(\"Top 10 Destinations for Departures:\")\n",
    "    top_10_destinations.show(truncate=False)\n",
    "\n",
    "    # Optionally, you can save the results to a file or another data source\n",
    "    # top_10_destinations.write.format(\"parquet\").mode(\"overwrite\").save(\"path/to/save/top_destinations\")\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
