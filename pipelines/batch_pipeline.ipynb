{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b661130b-6794-4895-a79d-15b23b05e4e6",
   "metadata": {},
   "source": [
    "Set dataset file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6629e93c-d053-4b59-b3b6-0537f21cde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/jovyan/data/flight_data/2023-10-03.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59247df5-d3d5-4804-a088-6226d443c541",
   "metadata": {},
   "source": [
    "Create sparksession and load flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b7a29a05-f329-4dd7-af60-497965ef80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightBatchProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "flight_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9898e-4125-43d3-ad2d-739c0c007d60",
   "metadata": {},
   "source": [
    "Count the number of incoming and outgoing flights for the entire day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "acdc8556-61d7-41a1-907d-ed103d1433d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|flightDirection|count|\n",
      "+---------------+-----+\n",
      "|              D| 2412|\n",
      "|              A| 2388|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of arrival and departure flights\n",
    "flight_counts = flight_data.groupBy(\"flightDirection\").count()\n",
    "\n",
    "# Show the results\n",
    "flight_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb1b36-e762-4e0a-bf0f-9194f2918352",
   "metadata": {},
   "source": [
    "Calculate the average difference between the actual and estimated landing time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4c9e9948-a29f-411a-814c-21f63f2dff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average landing time offset: 6.21 seconds\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "\n",
    "# Convert timestamp columns to Spark TimestampType\n",
    "flight_data = flight_data \\\n",
    "            .withColumn(\"actualLandingTime\", col(\"actualLandingTime\") \\\n",
    "            .cast(\"timestamp\")) \\\n",
    "            .withColumn(\"estimatedLandingTime\", col(\"estimatedLandingTime\") \\\n",
    "            .cast(\"timestamp\"))\n",
    "\n",
    "# Calculate the offset between estimated and actual landing times\n",
    "landing_time_offset = flight_data \\\n",
    "    .withColumn(\"landingTimeOffset\", abs(col(\"actualLandingTime\").cast(\"long\") - col(\"estimatedLandingTime\").cast(\"long\")))  # Convert seconds to minutes\n",
    "\n",
    "# Calculate the average offset\n",
    "average_offset = landing_time_offset.agg({\"landingTimeOffset\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "# Show the results\n",
    "print(f\"Average landing time offset: {average_offset:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151562eb-3dea-4035-bd4d-4caee9b2d0c7",
   "metadata": {},
   "source": [
    "Check gate and pier utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0440e788-d60f-4b0f-8885-62070f46423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departure Gate Utilization:\n",
      "+----+-------------------+\n",
      "|gate|gatePercentage     |\n",
      "+----+-------------------+\n",
      "|D81 |0.24875621890547264|\n",
      "|C6  |1.285240464344942  |\n",
      "|D66 |0.9950248756218906 |\n",
      "|B34 |0.9950248756218906 |\n",
      "|C22 |0.12437810945273632|\n",
      "|D28 |0.41459369817578773|\n",
      "|D16 |0.41459369817578773|\n",
      "|H6  |0.16583747927031509|\n",
      "|D5  |0.08291873963515754|\n",
      "|D7  |0.20729684908789386|\n",
      "|B22 |1.6998341625207296 |\n",
      "|B30 |2.155887230514096  |\n",
      "|D29 |0.08291873963515754|\n",
      "|D87 |0.8706467661691543 |\n",
      "|E9  |0.3731343283582089 |\n",
      "|M4  |0.24875621890547264|\n",
      "|D27 |0.12437810945273632|\n",
      "|D53 |0.16583747927031509|\n",
      "|B4  |1.6998341625207296 |\n",
      "|H1  |0.24875621890547264|\n",
      "+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Departure Pier Utilization:\n",
      "+-------+------------------+\n",
      "|pier   |pierPercentage    |\n",
      "+-------+------------------+\n",
      "|F      |3.316749585406302 |\n",
      "|E      |4.1459369817578775|\n",
      "|B      |31.38474295190713 |\n",
      "|M      |0.9950248756218906|\n",
      "|Unknown|4.353233830845771 |\n",
      "|D      |36.81592039800995 |\n",
      "|C      |15.630182421227198|\n",
      "|G      |2.197346600331675 |\n",
      "|H      |1.1608623548922055|\n",
      "+-------+------------------+\n",
      "\n",
      "+----+---------+-------------------+----+---------+------------------+\n",
      "|gate|gateCount|     gatePercentage|pier|pierCount|    pierPercentage|\n",
      "+----+---------+-------------------+----+---------+------------------+\n",
      "|  F3|       12| 0.4975124378109453|   F|       80| 3.316749585406302|\n",
      "|  F4|        8|0.33167495854063017|   F|       80| 3.316749585406302|\n",
      "|  F9|       17| 0.7048092868988391|   F|       80| 3.316749585406302|\n",
      "|  F5|       10|0.41459369817578773|   F|       80| 3.316749585406302|\n",
      "|  F7|       14| 0.5804311774461027|   F|       80| 3.316749585406302|\n",
      "|  F6|        9| 0.3731343283582089|   F|       80| 3.316749585406302|\n",
      "|  F8|       10|0.41459369817578773|   F|       80| 3.316749585406302|\n",
      "| E21|        5|0.20729684908789386|   E|      100|4.1459369817578775|\n",
      "| E18|        8|0.33167495854063017|   E|      100|4.1459369817578775|\n",
      "|  E2|       12| 0.4975124378109453|   E|      100|4.1459369817578775|\n",
      "| E17|        3|0.12437810945273632|   E|      100|4.1459369817578775|\n",
      "| E22|        4|0.16583747927031509|   E|      100|4.1459369817578775|\n",
      "| E20|        9| 0.3731343283582089|   E|      100|4.1459369817578775|\n",
      "| E24|        6|0.24875621890547264|   E|      100|4.1459369817578775|\n",
      "|  E3|       14| 0.5804311774461027|   E|      100|4.1459369817578775|\n",
      "|  E5|       12| 0.4975124378109453|   E|      100|4.1459369817578775|\n",
      "| E19|        6|0.24875621890547264|   E|      100|4.1459369817578775|\n",
      "|  E7|        8|0.33167495854063017|   E|      100|4.1459369817578775|\n",
      "|  E4|        4|0.16583747927031509|   E|      100|4.1459369817578775|\n",
      "|  E9|        9| 0.3731343283582089|   E|      100|4.1459369817578775|\n",
      "+----+---------+-------------------+----+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "\n",
    "# Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Group by gate and pier, and count the number of departure flights for each\n",
    "gate_counts = departure_data.groupBy(\"gate\").count().withColumnRenamed(\"count\", \"gateCount\")\n",
    "pier_counts = departure_data.groupBy(\"pier\").count().withColumnRenamed(\"count\", \"pierCount\")\n",
    "\n",
    "# Calculate the total number of non-null departure flights\n",
    "total_departure_flights = departure_data.na.drop(subset=[\"gate\", \"pier\"]).count()\n",
    "\n",
    "# Calculate the percentage of gate utilization for departures\n",
    "gate_utilization = gate_counts.withColumn(\"gatePercentage\", (col(\"gateCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Calculate the percentage of pier utilization for departures\n",
    "pier_utilization = pier_counts.withColumn(\"pierPercentage\", (col(\"pierCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Show the results\n",
    "print(\"Departure Gate Utilization:\")\n",
    "gate_utilization.select(\"gate\", \"gatePercentage\").show(truncate=False)\n",
    "\n",
    "print(\"Departure Pier Utilization:\")\n",
    "pier_utilization.select(\"pier\", \"pierPercentage\").show(truncate=False)\n",
    "\n",
    "# Define the condition for joining based on the first letter of the gate\n",
    "condition = col(\"gate\").substr(1, 1) == col(\"pier\")\n",
    "\n",
    "# Perform the join\n",
    "result_df = gate_utilization.join(pier_utilization, condition, \"inner\")\n",
    "\n",
    "# Show the result DataFrame\n",
    "result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b286fc-9406-46a6-bd96-f091aaf56f21",
   "metadata": {},
   "source": [
    "Check the ratio of charter and passenger flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d25afc0c-9552-4313-8cb6-7b7cc3b26723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Type Utilization for Departures:\n",
      "+-----------+-------------------+\n",
      "|serviceType|percentage         |\n",
      "+-----------+-------------------+\n",
      "|F          |0.8291873963515755 |\n",
      "|C          |0.4975124378109453 |\n",
      "|J          |98.424543946932    |\n",
      "|P          |0.24875621890547264|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Group by service type and count the number of departure flights for each\n",
    "service_type_counts = departure_data.groupBy(\"serviceType\").count().withColumnRenamed(\"count\", \"serviceTypeCount\")\n",
    "\n",
    "# Calculate the total number of departure flights\n",
    "total_departure_flights = departure_data.count()\n",
    "\n",
    "# Calculate the percentage of service type utilization for departures\n",
    "service_type_percentage = service_type_counts.withColumn(\"percentage\", (col(\"serviceTypeCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Show the results\n",
    "print(\"Service Type Utilization for Departures:\")\n",
    "service_type_percentage.select(\"serviceType\", \"percentage\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd254a-df04-4285-9888-f3ae8f6398d4",
   "metadata": {},
   "source": [
    "Define a udf so that in the next code block we can change the string representation of the route dict into a dict again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3350202c-d8f9-4354-afb3-74c0f31638ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, MapType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define a UDF to parse the string and convert it to a dictionary\n",
    "def string_to_dict_or_list(s):\n",
    "    import ast\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Register the UDF\n",
    "udf_string_to_dict = udf(string_to_dict_or_list, MapType(StringType(), StringType()))\n",
    "udf_string_to_list = udf(string_to_dict_or_list, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8153801-641c-414e-845e-febea66cea04",
   "metadata": {},
   "source": [
    "Check for the top 10 destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "72050fc5-3f0b-4ecf-97e3-1fd637a13974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Destinations for Departures:\n",
      "+-----------+-----------+\n",
      "|destination|flightCount|\n",
      "+-----------+-----------+\n",
      "|[CDG]      |98         |\n",
      "|[MAN]      |72         |\n",
      "|[LHR]      |71         |\n",
      "|[CPH]      |70         |\n",
      "|[ARN]      |59         |\n",
      "|[MAD]      |56         |\n",
      "|[OSL]      |51         |\n",
      "|[FCO]      |49         |\n",
      "|[BER]      |47         |\n",
      "|[MUC]      |44         |\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, from_json, split\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Convert the string representation of the route to a dictionary\n",
    "departure_data = departure_data.withColumn(\"route_dict\", udf_string_to_dict(departure_data[\"route\"]))\n",
    "\n",
    "# Split the destinations string into an array\n",
    "departure_data = departure_data.withColumn(\"destinations_array\", split(col(\"route_dict.destinations\"), \",\"))\n",
    "\n",
    "# Explode the destinations array to have one row per destination\n",
    "destinations_data = departure_data.select(\"destinations_array\").withColumn(\"destination\", explode(\"destinations_array\"))\n",
    "\n",
    "# Group by destination and count the number of departure flights for each\n",
    "destination_counts = destinations_data.groupBy(\"destination\").count().withColumnRenamed(\"count\", \"flightCount\")\n",
    "\n",
    "# Sort the destinations by flight count in descending order\n",
    "sorted_destinations = destination_counts.orderBy(col(\"flightCount\").desc())\n",
    "\n",
    "# Take the top 10 destinations\n",
    "top_10_destinations = sorted_destinations.limit(10)\n",
    "\n",
    "# Show the results\n",
    "print(\"Top 10 Destinations for Departures:\")\n",
    "top_10_destinations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0e4e5dca-f86e-487d-9465-fb69de7468c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_dests = \"/home/jovyan/data/destinations_data/destinations_with_coords.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6702cd96-f8d7-441a-bd2c-e72b9d507796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------+----+------------------+------------------+----------+\n",
      "|_c0|        city|       country|iata|        publicName|         longitude|  latitude|\n",
      "+---+------------+--------------+----+------------------+------------------+----------+\n",
      "|  0|    Al-Arish|         Egypt| AAC|          Al-Arish|        33.8032762| 31.132093|\n",
      "|  1|      Annaba|       Algeria| AAE|            Annaba|7.7500122000000005| 36.897375|\n",
      "|  2|Apalachicola|           USA| AAF|     Municipal, FL|       -84.9832435|29.7257675|\n",
      "|  3|      Aachen|       Germany| AAH|        Merzbrueck| 6.083886800000001|50.7753455|\n",
      "|  4|     Aalborg|       Danmark| AAL|           Aalborg|          9.921747|57.0488195|\n",
      "|  5|      Al Ain|United Arab Em| AAN|            Al Ain|        55.8023118|24.1301619|\n",
      "|  6|     Houston|           USA| AAP|Andrau Airpark, TX|       -95.3698028|29.7604267|\n",
      "|  7|       Anapa|  Russia (CIS)| AAQ|         Vitiazevo|        37.3158041|44.8935914|\n",
      "|  8|      Aarhus|       Danmark| AAR|            Aarhus|         10.203921| 56.162939|\n",
      "|  9|      Abakan|        Russia| ABA|            Abakan|        91.4293172|53.7175644|\n",
      "| 10|    Albacete|         Spain| ABC|          Albacete|         -1.860173|38.9942576|\n",
      "| 11|      Abadan|          Iran| ABD|            Abadan|        48.2754711|30.3666414|\n",
      "| 12|   Allentown|           USA| ABE|         Allentown|       -75.4714098|40.6022939|\n",
      "| 13|     Abilene|           USA| ABI|           Abilene|-99.73314390000002|32.4487364|\n",
      "| 14|     Abidjan|    Ivorycoast| ABJ|           Abidjan|        -4.0082563| 5.3599517|\n",
      "| 15| Albuquerque|           USA| ABQ|       Albuquerque|       -106.650422|35.0843859|\n",
      "| 16|    Aberdeen|           USA| ABR|      Regional, SD|        -2.0937528|57.1498891|\n",
      "| 17|  Abu Simpel|         Egypt| ABS|        ABU SIMPEL|        31.6156242|22.3460086|\n",
      "| 18|   Al-Bayaah|  Saudi Arabia| ABT|         Al-bayaah|       106.2479843|-6.9330665|\n",
      "| 19|     Atambua|     Indonesia| ABU|           Haliwen|       124.8987786|-9.1064894|\n",
      "+---+------------+--------------+----+------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|          route_dict|\n",
      "+--------------------+\n",
      "|{eu -> N, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> E, visa ->...|\n",
      "|{eu -> E, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[('lastUpdatedAt', 'string'), ('actualLandingTime', 'timestamp'), ('actualOffBlockTime', 'string'), ('aircraftRegistration', 'string'), ('aircraftType', 'string'), ('baggageClaim', 'string'), ('checkinAllocations', 'string'), ('codeshares', 'string'), ('estimatedLandingTime', 'timestamp'), ('expectedTimeBoarding', 'string'), ('expectedTimeGateClosing', 'string'), ('expectedTimeGateOpen', 'string'), ('expectedTimeOnBelt', 'string'), ('expectedSecurityFilter', 'string'), ('flightDirection', 'string'), ('flightName', 'string'), ('flightNumber', 'string'), ('gate', 'string'), ('pier', 'string'), ('id', 'string'), ('isOperationalFlight', 'string'), ('mainFlight', 'string'), ('prefixIATA', 'string'), ('prefixICAO', 'string'), ('airlineCode', 'string'), ('publicEstimatedOffBlockTime', 'string'), ('publicFlightState', 'string'), ('route', 'string'), ('scheduleDateTime', 'timestamp'), ('scheduleDate', 'string'), ('scheduleTime', 'string'), ('serviceType', 'string'), ('terminal', 'string'), ('transferPositions', 'string'), ('schemaVersion', 'string'), ('timeDifference', 'bigint'), ('route_dict', 'map<string,string>'), ('destinations', 'string'), ('destinations_list', 'array<string>')]\n",
      "+------------+\n",
      "|destinations|\n",
      "+------------+\n",
      "|       [DLM]|\n",
      "|       [PVK]|\n",
      "|       [ZTH]|\n",
      "|       [BCN]|\n",
      "|       [PMI]|\n",
      "|       [PMI]|\n",
      "|       [ACE]|\n",
      "|       [OPO]|\n",
      "|       [OPO]|\n",
      "|       [NAP]|\n",
      "|       [NAP]|\n",
      "|       [LCA]|\n",
      "|       [LCA]|\n",
      "|       [CHQ]|\n",
      "|       [PRG]|\n",
      "|       [PRG]|\n",
      "|       [PRG]|\n",
      "|       [PRG]|\n",
      "|       [LJU]|\n",
      "|       [LJU]|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_162/379580448.py\", line 7, in string_to_dict_or_list\n  File \"/opt/conda/lib/python3.11/ast.py\", line 110, in literal_eval\n    return _convert(node_or_string)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 90, in _convert\n    return list(map(_convert, node.elts))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 109, in _convert\n    return _convert_signed_num(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 83, in _convert_signed_num\n    return _convert_num(node)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 74, in _convert_num\n    _raise_malformed_node(node)\n  File \"/opt/conda/lib/python3.11/ast.py\", line 71, in _raise_malformed_node\n    raise ValueError(msg + f': {node!r}')\nValueError: malformed node or string on line 1: <ast.Name object at 0x7faefaad4940>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(departure_data\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[1;32m     28\u001b[0m departure_data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdestinations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mdeparture_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdestinations_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_162/379580448.py\", line 7, in string_to_dict_or_list\n  File \"/opt/conda/lib/python3.11/ast.py\", line 110, in literal_eval\n    return _convert(node_or_string)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 90, in _convert\n    return list(map(_convert, node.elts))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 109, in _convert\n    return _convert_signed_num(node)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 83, in _convert_signed_num\n    return _convert_num(node)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/lib/python3.11/ast.py\", line 74, in _convert_num\n    _raise_malformed_node(node)\n  File \"/opt/conda/lib/python3.11/ast.py\", line 71, in _raise_malformed_node\n    raise ValueError(msg + f': {node!r}')\nValueError: malformed node or string on line 1: <ast.Name object at 0x7faefaad4940>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs\n",
    "\n",
    "\n",
    "# Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "destinations_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path_dests)\n",
    "\n",
    "destinations_data.show()\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Convert the string representation of the route to a dictionary\n",
    "departure_data = departure_data.withColumn(\"route_dict\", udf_string_to_dict(departure_data[\"route\"]))\n",
    "\n",
    "departure_data.select(\"route_dict\").show()\n",
    "\n",
    "# Create a new column 'new_column' with the desired values\n",
    "departure_data = departure_data.withColumn('destinations', col('route_dict').getItem(desired_key))\n",
    "\n",
    "departure_data = departure_data.withColumn(\"destinations_list\", udf_string_to_list(departure_data[\"destinations\"]))\n",
    "\n",
    "print(departure_data.dtypes)\n",
    "\n",
    "departure_data.select(\"destinations\").show()\n",
    "departure_data.select(\"destinations_list\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "22fe5b98-dba1-4854-adfc-9e89a4c9c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
