{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f37dc24-327a-46c5-84f5-23497d445c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.13.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting grpcio<2.0dev,>=1.47.0 (from google-cloud-bigquery)\n",
      "  Downloading grpcio-1.59.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.15.0 (from google-cloud-bigquery)\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0 (from google-cloud-bigquery)\n",
      "  Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery)\n",
      "  Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (23.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading google_auth-2.24.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading grpcio_status-1.59.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery)\n",
      "  Downloading google_crc32c-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading google_cloud_bigquery-3.13.0-py2.py3-none-any.whl (222 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.14.0-py3-none-any.whl (122 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.59.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.24.0-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.8/183.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.59.3-py3-none-any.whl (14 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyasn1, proto-plus, grpcio, googleapis-common-protos, google-crc32c, cachetools, rsa, pyasn1-modules, grpcio-status, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "Successfully installed cachetools-5.3.2 google-api-core-2.14.0 google-auth-2.24.0 google-cloud-bigquery-3.13.0 google-cloud-core-2.3.3 google-crc32c-1.5.0 google-resumable-media-2.6.0 googleapis-common-protos-1.61.0 grpcio-1.59.3 grpcio-status-1.59.3 proto-plus-1.22.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 rsa-4.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661130b-6794-4895-a79d-15b23b05e4e6",
   "metadata": {},
   "source": [
    "Set dataset file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6629e93c-d053-4b59-b3b6-0537f21cde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'gs://schiphol-flight-data-bucket/flight_data/2023-10-04.csv'\n",
    "file_path_dests = 'gs://schiphol-flight-data-bucket/destination_data/destinations_with_coords.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59247df5-d3d5-4804-a088-6226d443c541",
   "metadata": {},
   "source": [
    "Create sparksession and load flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7a29a05-f329-4dd7-af60-497965ef80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightBatchProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "flight_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9898e-4125-43d3-ad2d-739c0c007d60",
   "metadata": {},
   "source": [
    "Count the number of incoming and outgoing flights for the entire day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acdc8556-61d7-41a1-907d-ed103d1433d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----+\n",
      "|flightDirection|scheduleDate|count|\n",
      "+---------------+------------+-----+\n",
      "|              D|  2023-10-04| 2466|\n",
      "|              A|  2023-10-04| 2434|\n",
      "+---------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of arrival and departure flights\n",
    "flight_counts = flight_data.groupBy(\"flightDirection\", \"scheduleDate\").count()\n",
    "\n",
    "# Show the results\n",
    "flight_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb1b36-e762-4e0a-bf0f-9194f2918352",
   "metadata": {},
   "source": [
    "Calculate the average difference between the actual and estimated landing time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c9e9948-a29f-411a-814c-21f63f2dff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|scheduleDate|avg(landingTimeOffset)|\n",
      "+------------+----------------------+\n",
      "|  2023-10-04|    10.772425249169435|\n",
      "+------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "\n",
    "# Convert timestamp columns to Spark TimestampType\n",
    "flight_data = flight_data \\\n",
    "            .withColumn(\"actualLandingTime\", col(\"actualLandingTime\") \\\n",
    "            .cast(\"timestamp\")) \\\n",
    "            .withColumn(\"estimatedLandingTime\", col(\"estimatedLandingTime\") \\\n",
    "            .cast(\"timestamp\"))\n",
    "\n",
    "# Calculate the offset between estimated and actual landing times\n",
    "landing_time_offset = flight_data \\\n",
    "    .withColumn(\"landingTimeOffset\", abs(col(\"actualLandingTime\").cast(\"long\") - col(\"estimatedLandingTime\").cast(\"long\")))  # Convert seconds to minutes\n",
    "\n",
    "# Calculate the average offset\n",
    "average_offset = landing_time_offset.groupBy(\"scheduleDate\").agg({\"landingTimeOffset\": \"avg\"})\n",
    "\n",
    "# Show the results\n",
    "average_offset.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151562eb-3dea-4035-bd4d-4caee9b2d0c7",
   "metadata": {},
   "source": [
    "Check gate and pier utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0440e788-d60f-4b0f-8885-62070f46423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departure Gate Utilization:\n",
      "+----+-------------------+------------+\n",
      "|gate|gatePercentage     |scheduleDate|\n",
      "+----+-------------------+------------+\n",
      "|C10 |0.8921330089213302 |2023-10-04  |\n",
      "|D6  |10.908353609083536 |2023-10-04  |\n",
      "|M1  |0.16220600162206003|2023-10-04  |\n",
      "|D29 |0.9732360097323601 |2023-10-04  |\n",
      "|D24 |0.36496350364963503|2023-10-04  |\n",
      "|C15 |1.094890510948905  |2023-10-04  |\n",
      "|D2  |0.48661800486618007|2023-10-04  |\n",
      "|E9  |0.4460665044606651 |2023-10-04  |\n",
      "|D79 |0.5677210056772101 |2023-10-04  |\n",
      "|B3  |0.8110300081103    |2023-10-04  |\n",
      "|C18 |1.2165450121654502 |2023-10-04  |\n",
      "|B36 |0.9732360097323601 |2023-10-04  |\n",
      "|B24 |1.5409570154095702 |2023-10-04  |\n",
      "|D84 |1.257096512570965  |2023-10-04  |\n",
      "|B34 |1.7437145174371453 |2023-10-04  |\n",
      "|C5  |2.4330900243309004 |2023-10-04  |\n",
      "|D59 |0.527169505271695  |2023-10-04  |\n",
      "|D10 |0.24330900243309003|2023-10-04  |\n",
      "|B27 |1.05433901054339   |2023-10-04  |\n",
      "|M3  |0.28386050283860503|2023-10-04  |\n",
      "+----+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Departure Pier Utilization:\n",
      "+-------+------------------+------------+\n",
      "|pier   |pierPercentage    |scheduleDate|\n",
      "+-------+------------------+------------+\n",
      "|B      |31.995133819951338|2023-10-04  |\n",
      "|F      |3.0819140308191404|2023-10-04  |\n",
      "|G      |2.5952960259529605|2023-10-04  |\n",
      "|E      |4.5417680454176805|2023-10-04  |\n",
      "|M      |1.05433901054339  |2023-10-04  |\n",
      "|C      |15.044606650446065|2023-10-04  |\n",
      "|H      |1.1354420113544201|2023-10-04  |\n",
      "|Unknown|1.2976480129764802|2023-10-04  |\n",
      "|D      |39.253852392538526|2023-10-04  |\n",
      "+-------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "\n",
    "# Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Group by gate and pier, and count the number of departure flights for each\n",
    "gate_counts = departure_data.groupBy(\"gate\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"gateCount\")\n",
    "pier_counts = departure_data.groupBy(\"pier\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"pierCount\")\n",
    "\n",
    "# Calculate the total number of non-null departure flights\n",
    "total_departure_flights = departure_data.na.drop(subset=[\"gate\", \"pier\"]).count()\n",
    "\n",
    "# Calculate the percentage of gate utilization for departures\n",
    "gate_utilization = gate_counts.withColumn(\"gatePercentage\", (col(\"gateCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Calculate the percentage of pier utilization for departures\n",
    "pier_utilization = pier_counts.withColumn(\"pierPercentage\", (col(\"pierCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Show the results\n",
    "print(\"Departure Gate Utilization:\")\n",
    "gate_table = gate_utilization.select(\"gate\", \"gatePercentage\", \"scheduleDate\")\n",
    "\n",
    "gate_table.show(truncate=False)\n",
    "\n",
    "print(\"Departure Pier Utilization:\")\n",
    "pier_table = pier_utilization.select(\"pier\", \"pierPercentage\", \"scheduleDate\")\n",
    "\n",
    "pier_table.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b286fc-9406-46a6-bd96-f091aaf56f21",
   "metadata": {},
   "source": [
    "Check the ratio of charter and passenger flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d25afc0c-9552-4313-8cb6-7b7cc3b26723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Type Utilization for Departures:\n",
      "+-----------+-------------------+------------+\n",
      "|serviceType|percentage         |scheduleDate|\n",
      "+-----------+-------------------+------------+\n",
      "|F          |1.1354420113544201 |2023-10-04  |\n",
      "|C          |0.48661800486618007|2023-10-04  |\n",
      "|H          |0.04055150040551501|2023-10-04  |\n",
      "|J          |98.29683698296837  |2023-10-04  |\n",
      "|P          |0.04055150040551501|2023-10-04  |\n",
      "+-----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Group by service type and count the number of departure flights for each\n",
    "service_type_counts = departure_data.groupBy(\"serviceType\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"serviceTypeCount\")\n",
    "\n",
    "# Calculate the total number of departure flights\n",
    "total_departure_flights = departure_data.count()\n",
    "\n",
    "# Calculate the percentage of service type utilization for departures\n",
    "service_type_percentage = service_type_counts.withColumn(\"percentage\", (col(\"serviceTypeCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Show the results\n",
    "print(\"Service Type Utilization for Departures:\")\n",
    "service_type_table = service_type_percentage.select(\"serviceType\", \"percentage\", \"scheduleDate\")\n",
    "service_type_table.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd254a-df04-4285-9888-f3ae8f6398d4",
   "metadata": {},
   "source": [
    "Define a udf so that in the next code block we can change the string representation of the route dict into a dict again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3350202c-d8f9-4354-afb3-74c0f31638ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, MapType, ArrayType, StructType, StructField, BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define a UDF to parse the string and convert it to a dictionary\n",
    "def string_to_dict_or_list(s):\n",
    "    import ast\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Register the UDF\n",
    "udf_string_to_dict = udf(string_to_dict_or_list, MapType(StringType(), StringType()))\n",
    "udf_string_to_list = udf(string_to_dict_or_list, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8153801-641c-414e-845e-febea66cea04",
   "metadata": {},
   "source": [
    "Check for the top 10 destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "72050fc5-3f0b-4ecf-97e3-1fd637a13974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Destinations for Departures:\n",
      "+-----------+------------+-----------+\n",
      "|destination|scheduleDate|flightCount|\n",
      "+-----------+------------+-----------+\n",
      "|[CDG]      |2023-10-04  |98         |\n",
      "|[MAN]      |2023-10-04  |73         |\n",
      "|[LHR]      |2023-10-04  |72         |\n",
      "|[CPH]      |2023-10-04  |70         |\n",
      "|[ARN]      |2023-10-04  |59         |\n",
      "|[OSL]      |2023-10-04  |51         |\n",
      "|[MAD]      |2023-10-04  |50         |\n",
      "|[BER]      |2023-10-04  |48         |\n",
      "|[BCN]      |2023-10-04  |48         |\n",
      "|[MUC]      |2023-10-04  |46         |\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, from_json, split\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Convert the string representation of the route to a dictionary\n",
    "departure_data = departure_data.withColumn(\"route_dict\", udf_string_to_dict(departure_data[\"route\"]))\n",
    "\n",
    "# Split the destinations string into an array\n",
    "departure_data = departure_data.withColumn(\"destinations_array\", split(col(\"route_dict.destinations\"), \",\"))\n",
    "\n",
    "# Explode the destinations array to have one row per destination\n",
    "destinations_data = departure_data.select(\"destinations_array\", \"scheduleDate\").withColumn(\"destination\", explode(\"destinations_array\"))\n",
    "\n",
    "# Group by destination and count the number of departure flights for each\n",
    "destination_counts = destinations_data.groupBy(\"destination\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"flightCount\")\n",
    "\n",
    "# Sort the destinations by flight count in descending order\n",
    "sorted_destinations = destination_counts.orderBy(col(\"flightCount\").desc())\n",
    "\n",
    "# Take the top 10 destinations\n",
    "top_10_destinations = sorted_destinations.limit(10)\n",
    "\n",
    "# Show the results\n",
    "print(\"Top 10 Destinations for Departures:\")\n",
    "top_10_destinations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cd2cae-3cff-482f-abe1-4578e59f9abd",
   "metadata": {},
   "source": [
    "Load destinations data in order to be sent to bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "917b81b9-3003-4275-b329-0f7bdc80dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+----------+\n",
      "|iata|         longitude|  latitude|\n",
      "+----+------------------+----------+\n",
      "| AAC|        33.8032762| 31.132093|\n",
      "| AAE|7.7500122000000005| 36.897375|\n",
      "| AAF|       -84.9832435|29.7257675|\n",
      "| AAH| 6.083886800000001|50.7753455|\n",
      "| AAL|          9.921747|57.0488195|\n",
      "| AAN|        55.8023118|24.1301619|\n",
      "| AAP|       -95.3698028|29.7604267|\n",
      "| AAQ|        37.3158041|44.8935914|\n",
      "| AAR|         10.203921| 56.162939|\n",
      "| ABA|        91.4293172|53.7175644|\n",
      "| ABC|         -1.860173|38.9942576|\n",
      "| ABD|        48.2754711|30.3666414|\n",
      "| ABE|       -75.4714098|40.6022939|\n",
      "| ABI|-99.73314390000002|32.4487364|\n",
      "| ABJ|        -4.0082563| 5.3599517|\n",
      "| ABQ|       -106.650422|35.0843859|\n",
      "| ABR|        -2.0937528|57.1498891|\n",
      "| ABS|        31.6156242|22.3460086|\n",
      "| ABT|       106.2479843|-6.9330665|\n",
      "| ABU|       124.8987786|-9.1064894|\n",
      "+----+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "destinations_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path_dests)\n",
    "\n",
    "destinations_data = destinations_data.select(\"iata\", \"longitude\", \"latitude\")\n",
    "\n",
    "destinations_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6702cd96-f8d7-441a-bd2c-e72b9d507796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|destinations|\n",
      "+------------+\n",
      "|       [LEJ]|\n",
      "|       [ZTH]|\n",
      "|       [RAK]|\n",
      "|       [RAK]|\n",
      "|       [ALC]|\n",
      "|       [ALC]|\n",
      "|       [KLX]|\n",
      "|       [BCN]|\n",
      "|       [OPO]|\n",
      "|       [OPO]|\n",
      "|       [CFU]|\n",
      "|       [TFS]|\n",
      "|       [TFS]|\n",
      "|       [ZTH]|\n",
      "|       [OLB]|\n",
      "|       [OLB]|\n",
      "|       [FNC]|\n",
      "|       [FNC]|\n",
      "|       [FNC]|\n",
      "|       [TFS]|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Convert the string representation of the route to a dictionary\n",
    "departure_data = departure_data.withColumn(\"route_dict\", udf_string_to_dict(departure_data[\"route\"]))\n",
    "\n",
    "# Create a new column 'new_column' with the desired values\n",
    "departure_data = departure_data.withColumn('destinations', col('route_dict').getItem('destinations'))\n",
    "\n",
    "departure_data = departure_data.withColumn(\"destinations_list\", udf_string_to_list(departure_data[\"destinations\"]))\n",
    "\n",
    "departure_data.select(\"destinations\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec42c210-5fce-4ebc-99d9-0b6716c93878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"tmp-bucket-for-data-engineering\"  # use your bucket \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "flight_counts.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.flight_counts') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n",
    "\n",
    "top_10_destinations.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.popular_dests') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n",
    "\n",
    "gate_table.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.gate_utilization') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n",
    "\n",
    "pier_table.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.pier_utilization') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n",
    "\n",
    "service_type_table.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.service_type_utilization') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()\n",
    "\n",
    "destinations_data.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.destinations_data') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "22fe5b98-dba1-4854-adfc-9e89a4c9c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
