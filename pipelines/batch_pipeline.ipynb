{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f37dc24-327a-46c5-84f5-23497d445c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.11/site-packages (3.13.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (1.59.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.14.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (1.22.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.6.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (23.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.61.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.23.4)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.59.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661130b-6794-4895-a79d-15b23b05e4e6",
   "metadata": {},
   "source": [
    "Set dataset file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6629e93c-d053-4b59-b3b6-0537f21cde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'gs://schiphol-flight-data-bucket/flight_data/2023-10-04.csv'\n",
    "file_path_dests = 'gs://schiphol-flight-data-bucket/destination_data/destinations_with_coords.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59247df5-d3d5-4804-a088-6226d443c541",
   "metadata": {},
   "source": [
    "Create sparksession and load flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7a29a05-f329-4dd7-af60-497965ef80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightBatchProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "flight_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9898e-4125-43d3-ad2d-739c0c007d60",
   "metadata": {},
   "source": [
    "Count the number of incoming and outgoing flights for the entire day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acdc8556-61d7-41a1-907d-ed103d1433d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----+\n",
      "|flightDirection|scheduleDate|count|\n",
      "+---------------+------------+-----+\n",
      "|              D|  2023-10-01| 2348|\n",
      "|              A|  2023-10-01| 2332|\n",
      "+---------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of arrival and departure flights\n",
    "flight_counts = flight_data.groupBy(\"flightDirection\", \"scheduleDate\").count()\n",
    "\n",
    "# Show the results\n",
    "flight_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb1b36-e762-4e0a-bf0f-9194f2918352",
   "metadata": {},
   "source": [
    "Calculate the average difference between the actual and estimated landing time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9e9948-a29f-411a-814c-21f63f2dff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|scheduleDate|avg(landingTimeOffset)|\n",
      "+------------+----------------------+\n",
      "|  2023-10-01|    13.884649122807017|\n",
      "+------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql.functions import abs\n",
    "\n",
    "# Convert timestamp columns to Spark TimestampType\n",
    "flight_data = flight_data \\\n",
    "            .withColumn(\"actualLandingTime\", col(\"actualLandingTime\") \\\n",
    "            .cast(\"timestamp\")) \\\n",
    "            .withColumn(\"estimatedLandingTime\", col(\"estimatedLandingTime\") \\\n",
    "            .cast(\"timestamp\"))\n",
    "\n",
    "# Calculate the offset between estimated and actual landing times\n",
    "landing_time_offset = flight_data \\\n",
    "    .withColumn(\"landingTimeOffset\", abs(col(\"actualLandingTime\").cast(\"long\") - col(\"estimatedLandingTime\").cast(\"long\")))  # Convert seconds to minutes\n",
    "\n",
    "# Calculate the average offset\n",
    "average_offset = landing_time_offset.groupBy(\"scheduleDate\").agg({\"landingTimeOffset\": \"avg\"})\n",
    "\n",
    "# Show the results\n",
    "average_offset.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151562eb-3dea-4035-bd4d-4caee9b2d0c7",
   "metadata": {},
   "source": [
    "Check gate and pier utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0440e788-d60f-4b0f-8885-62070f46423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Departure Gate Utilization:\n",
      "+----+-------------------+------------+\n",
      "|gate|gatePercentage     |scheduleDate|\n",
      "+----+-------------------+------------+\n",
      "|F8  |0.46848381601362865|2023-10-01  |\n",
      "|D78 |1.4906303236797274 |2023-10-01  |\n",
      "|G9  |0.2555366269165247 |2023-10-01  |\n",
      "|E21 |0.9369676320272573 |2023-10-01  |\n",
      "|D44 |0.2555366269165247 |2023-10-01  |\n",
      "|D63 |0.5110732538330494 |2023-10-01  |\n",
      "|B23 |0.6388415672913117 |2023-10-01  |\n",
      "|B27 |0.9795570698466781 |2023-10-01  |\n",
      "|B35 |0.7240204429301533 |2023-10-01  |\n",
      "|B5  |0.5536626916524702 |2023-10-01  |\n",
      "|C18 |1.6183986371379897 |2023-10-01  |\n",
      "|C23 |0.12776831345826234|2023-10-01  |\n",
      "|C6  |0.2555366269165247 |2023-10-01  |\n",
      "|B17 |0.42589437819420783|2023-10-01  |\n",
      "|D59 |0.596252129471891  |2023-10-01  |\n",
      "|B28 |1.7035775127768313 |2023-10-01  |\n",
      "|E5  |0.5536626916524702 |2023-10-01  |\n",
      "|H3  |0.17035775127768313|2023-10-01  |\n",
      "|D6  |9.412265758091994  |2023-10-01  |\n",
      "|B32 |1.2776831345826234 |2023-10-01  |\n",
      "+----+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Departure Pier Utilization:\n",
      "+-------+------------------+------------+\n",
      "|pier   |pierPercentage    |scheduleDate|\n",
      "+-------+------------------+------------+\n",
      "|C      |16.90800681431005 |2023-10-01  |\n",
      "|M      |1.2350936967632027|2023-10-01  |\n",
      "|D      |35.34923339011925 |2023-10-01  |\n",
      "|E      |5.9625212947189095|2023-10-01  |\n",
      "|H      |1.2776831345826234|2023-10-01  |\n",
      "|Unknown|2.2146507666098807|2023-10-01  |\n",
      "|G      |2.0442930153321974|2023-10-01  |\n",
      "|F      |3.3645655877342415|2023-10-01  |\n",
      "|B      |31.643952299829643|2023-10-01  |\n",
      "+-------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when\n",
    "\n",
    "\n",
    "# Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Group by gate and pier, and count the number of departure flights for each\n",
    "gate_counts = departure_data.groupBy(\"gate\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"gateCount\")\n",
    "pier_counts = departure_data.groupBy(\"pier\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"pierCount\")\n",
    "\n",
    "# Calculate the total number of non-null departure flights\n",
    "total_departure_flights = departure_data.na.drop(subset=[\"gate\", \"pier\"]).count()\n",
    "\n",
    "# Calculate the percentage of gate utilization for departures\n",
    "gate_utilization = gate_counts.withColumn(\"gatePercentage\", (col(\"gateCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Calculate the percentage of pier utilization for departures\n",
    "pier_utilization = pier_counts.withColumn(\"pierPercentage\", (col(\"pierCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Show the results\n",
    "print(\"Departure Gate Utilization:\")\n",
    "gate_table = gate_utilization.select(\"gate\", \"gatePercentage\", \"scheduleDate\")\n",
    "\n",
    "gate_table.show(truncate=False)\n",
    "\n",
    "print(\"Departure Pier Utilization:\")\n",
    "pier_table = pier_utilization.select(\"pier\", \"pierPercentage\", \"scheduleDate\")\n",
    "\n",
    "pier_table.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b286fc-9406-46a6-bd96-f091aaf56f21",
   "metadata": {},
   "source": [
    "Check the ratio of charter and passenger flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25afc0c-9552-4313-8cb6-7b7cc3b26723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Type Utilization for Departures:\n",
      "+-----------+-------------------+------------+\n",
      "|serviceType|percentage         |scheduleDate|\n",
      "+-----------+-------------------+------------+\n",
      "|C          |0.46848381601362865|2023-10-01  |\n",
      "|J          |98.50936967632026  |2023-10-01  |\n",
      "|F          |0.8517887563884157 |2023-10-01  |\n",
      "|P          |0.17035775127768313|2023-10-01  |\n",
      "+-----------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Replace null or NaN values with a placeholder value (e.g., \"Unknown\")\n",
    "flight_data = flight_data.na.fill(\"Unknown\")\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Group by service type and count the number of departure flights for each\n",
    "service_type_counts = departure_data.groupBy(\"serviceType\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"serviceTypeCount\")\n",
    "\n",
    "# Calculate the total number of departure flights\n",
    "total_departure_flights = departure_data.count()\n",
    "\n",
    "# Calculate the percentage of service type utilization for departures\n",
    "service_type_percentage = service_type_counts.withColumn(\"percentage\", (col(\"serviceTypeCount\") / total_departure_flights) * 100)\n",
    "\n",
    "# Show the results\n",
    "print(\"Service Type Utilization for Departures:\")\n",
    "service_type_table = service_type_percentage.select(\"serviceType\", \"percentage\", \"scheduleDate\")\n",
    "service_type_table.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd254a-df04-4285-9888-f3ae8f6398d4",
   "metadata": {},
   "source": [
    "Define a udf so that in the next code block we can change the string representation of the route dict into a dict again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3350202c-d8f9-4354-afb3-74c0f31638ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, MapType, ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define a UDF to parse the string and convert it to a dictionary\n",
    "def string_to_dict_or_list(s):\n",
    "    import ast\n",
    "    return ast.literal_eval(s)\n",
    "\n",
    "# Register the UDF\n",
    "udf_string_to_dict = udf(string_to_dict_or_list, MapType(StringType(), StringType()))\n",
    "udf_string_to_list = udf(string_to_dict_or_list, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8153801-641c-414e-845e-febea66cea04",
   "metadata": {},
   "source": [
    "Check for the top 10 destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72050fc5-3f0b-4ecf-97e3-1fd637a13974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Destinations for Departures:\n",
      "+-----------+------------+-----------+\n",
      "|destination|scheduleDate|flightCount|\n",
      "+-----------+------------+-----------+\n",
      "|[CDG]      |2023-10-01  |82         |\n",
      "|[CPH]      |2023-10-01  |71         |\n",
      "|[MAN]      |2023-10-01  |68         |\n",
      "|[LHR]      |2023-10-01  |57         |\n",
      "|[MAD]      |2023-10-01  |56         |\n",
      "|[ARN]      |2023-10-01  |51         |\n",
      "|[OSL]      |2023-10-01  |51         |\n",
      "|[FCO]      |2023-10-01  |49         |\n",
      "|[BCN]      |2023-10-01  |49         |\n",
      "|[BER]      |2023-10-01  |47         |\n",
      "+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, from_json, split\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Convert the string representation of the route to a dictionary\n",
    "departure_data = departure_data.withColumn(\"route_dict\", udf_string_to_dict(departure_data[\"route\"]))\n",
    "\n",
    "# Split the destinations string into an array\n",
    "departure_data = departure_data.withColumn(\"destinations_array\", split(col(\"route_dict.destinations\"), \",\"))\n",
    "\n",
    "# Explode the destinations array to have one row per destination\n",
    "destinations_data = departure_data.select(\"destinations_array\", \"scheduleDate\").withColumn(\"destination\", explode(\"destinations_array\"))\n",
    "\n",
    "# Group by destination and count the number of departure flights for each\n",
    "destination_counts = destinations_data.groupBy(\"destination\", \"scheduleDate\").count().withColumnRenamed(\"count\", \"flightCount\")\n",
    "\n",
    "# Sort the destinations by flight count in descending order\n",
    "sorted_destinations = destination_counts.orderBy(col(\"flightCount\").desc())\n",
    "\n",
    "# Take the top 10 destinations\n",
    "top_10_destinations = sorted_destinations.limit(10)\n",
    "\n",
    "# Show the results\n",
    "print(\"Top 10 Destinations for Departures:\")\n",
    "top_10_destinations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6702cd96-f8d7-441a-bd2c-e72b9d507796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------+----+------------------+------------------+----------+\n",
      "|_c0|        city|       country|iata|        publicName|         longitude|  latitude|\n",
      "+---+------------+--------------+----+------------------+------------------+----------+\n",
      "|  0|    Al-Arish|         Egypt| AAC|          Al-Arish|        33.8032762| 31.132093|\n",
      "|  1|      Annaba|       Algeria| AAE|            Annaba|7.7500122000000005| 36.897375|\n",
      "|  2|Apalachicola|           USA| AAF|     Municipal, FL|       -84.9832435|29.7257675|\n",
      "|  3|      Aachen|       Germany| AAH|        Merzbrueck| 6.083886800000001|50.7753455|\n",
      "|  4|     Aalborg|       Danmark| AAL|           Aalborg|          9.921747|57.0488195|\n",
      "|  5|      Al Ain|United Arab Em| AAN|            Al Ain|        55.8023118|24.1301619|\n",
      "|  6|     Houston|           USA| AAP|Andrau Airpark, TX|       -95.3698028|29.7604267|\n",
      "|  7|       Anapa|  Russia (CIS)| AAQ|         Vitiazevo|        37.3158041|44.8935914|\n",
      "|  8|      Aarhus|       Danmark| AAR|            Aarhus|         10.203921| 56.162939|\n",
      "|  9|      Abakan|        Russia| ABA|            Abakan|        91.4293172|53.7175644|\n",
      "| 10|    Albacete|         Spain| ABC|          Albacete|         -1.860173|38.9942576|\n",
      "| 11|      Abadan|          Iran| ABD|            Abadan|        48.2754711|30.3666414|\n",
      "| 12|   Allentown|           USA| ABE|         Allentown|       -75.4714098|40.6022939|\n",
      "| 13|     Abilene|           USA| ABI|           Abilene|-99.73314390000002|32.4487364|\n",
      "| 14|     Abidjan|    Ivorycoast| ABJ|           Abidjan|        -4.0082563| 5.3599517|\n",
      "| 15| Albuquerque|           USA| ABQ|       Albuquerque|       -106.650422|35.0843859|\n",
      "| 16|    Aberdeen|           USA| ABR|      Regional, SD|        -2.0937528|57.1498891|\n",
      "| 17|  Abu Simpel|         Egypt| ABS|        ABU SIMPEL|        31.6156242|22.3460086|\n",
      "| 18|   Al-Bayaah|  Saudi Arabia| ABT|         Al-bayaah|       106.2479843|-6.9330665|\n",
      "| 19|     Atambua|     Indonesia| ABU|           Haliwen|       124.8987786|-9.1064894|\n",
      "+---+------------+--------------+----+------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|          route_dict|\n",
      "+--------------------+\n",
      "|{eu -> N, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> N, visa ->...|\n",
      "|{eu -> N, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "|{eu -> S, visa ->...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+\n",
      "|destinations|\n",
      "+------------+\n",
      "|  [SHJ, SIN]|\n",
      "|       [OLB]|\n",
      "|       [OLB]|\n",
      "|       [AOK]|\n",
      "|       [AGP]|\n",
      "|       [AGP]|\n",
      "|       [RAK]|\n",
      "|       [RAK]|\n",
      "|       [MAH]|\n",
      "|       [CHQ]|\n",
      "|       [IBZ]|\n",
      "|       [IBZ]|\n",
      "|       [ALC]|\n",
      "|       [ALC]|\n",
      "|       [IBZ]|\n",
      "|       [KGS]|\n",
      "|       [BCN]|\n",
      "|       [PSA]|\n",
      "|       [PSA]|\n",
      "|       [FUE]|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs\n",
    "\n",
    "\n",
    "def extract_first_element(lst):\n",
    "    return str(lst[0]) if lst else None\n",
    "\n",
    "# Register the UDF\n",
    "extract_first_element_udf = udf(extract_first_element, StringType())\n",
    "\n",
    "\n",
    "# Load the flight data from your source (e.g., CSV, Parquet, etc.)\n",
    "destinations_data = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(file_path_dests)\n",
    "\n",
    "destinations_data.show()\n",
    "\n",
    "# Filter only departure flights\n",
    "departure_data = flight_data.filter(col(\"flightDirection\") == \"D\")\n",
    "\n",
    "# Convert the string representation of the route to a dictionary\n",
    "departure_data = departure_data.withColumn(\"route_dict\", udf_string_to_dict(departure_data[\"route\"]))\n",
    "\n",
    "departure_data.select(\"route_dict\").show()\n",
    "\n",
    "# Create a new column 'new_column' with the desired values\n",
    "departure_data = departure_data.withColumn('destinations', col('route_dict').getItem('destinations'))\n",
    "\n",
    "departure_data = departure_data.withColumn(\"destinations_list\", udf_string_to_list(departure_data[\"destinations\"]))\n",
    "\n",
    "departure_data.select(\"destinations\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec42c210-5fce-4ebc-99d9-0b6716c93878",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o224.save.\n: com.google.cloud.spark.bigquery.repackaged.com.google.inject.ProvisionException: Unable to provision, see the following errors:\n\n1) [Guice/ErrorInCustomProvider]: IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment.  Please set a project ID using the builder.\n  at SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:78)\n  while locating SparkBigQueryConfig\n\nLearn more:\n  https://github.com/google/guice/wiki/ERROR_IN_CUSTOM_PROVIDER\n\n1 error\n\n======================\nFull classname legend:\n======================\nSparkBigQueryConfig:          \"com.google.cloud.spark.bigquery.SparkBigQueryConfig\"\nSparkBigQueryConnectorModule: \"com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule\"\n========================\nEnd of classname legend:\n========================\n\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProvisionException.toProvisionException(InternalProvisionException.java:251)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1104)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1139)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelationInternal(CreatableRelationProviderHelper.java:114)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelation(CreatableRelationProviderHelper.java:95)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:47)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment.  Please set a project ID using the builder.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.common.base.Preconditions.checkArgument(Preconditions.java:143)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:304)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions.<init>(BigQueryOptions.java:92)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions.<init>(BigQueryOptions.java:30)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions$Builder.build(BigQueryOptions.java:87)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions.getDefaultInstance(BigQueryOptions.java:162)\n\tat com.google.cloud.bigquery.connector.common.BigQueryConfigurationUtil.lambda$defaultBilledProject$1(BigQueryConfigurationUtil.java:41)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.common.base.Absent.or(Absent.java:61)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConfig.from(SparkBigQueryConfig.java:330)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConfig.from(SparkBigQueryConfig.java:247)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule.lambda$provideSparkBigQueryConfig$0(SparkBigQueryConnectorModule.java:80)\n\tat java.base/java.util.Optional.orElseGet(Optional.java:364)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:78)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule$$FastClassByGuice$$1232932.GUICE$TRAMPOLINE(<generated>)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule$$FastClassByGuice$$1232932.apply(<generated>)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod$FastClassProviderMethod.doProvision(ProviderMethod.java:260)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod.doProvision(ProviderMethod.java:171)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.provision(InternalProviderInstanceBindingImpl.java:185)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.get(InternalProviderInstanceBindingImpl.java:162)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:169)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1101)\n\t... 46 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m conf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs.AbstractFileSystem.gs.impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Saving the data to BigQuery\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[43mflight_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbigquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mschiphol_data.flight_counts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 18\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o224.save.\n: com.google.cloud.spark.bigquery.repackaged.com.google.inject.ProvisionException: Unable to provision, see the following errors:\n\n1) [Guice/ErrorInCustomProvider]: IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment.  Please set a project ID using the builder.\n  at SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:78)\n  while locating SparkBigQueryConfig\n\nLearn more:\n  https://github.com/google/guice/wiki/ERROR_IN_CUSTOM_PROVIDER\n\n1 error\n\n======================\nFull classname legend:\n======================\nSparkBigQueryConfig:          \"com.google.cloud.spark.bigquery.SparkBigQueryConfig\"\nSparkBigQueryConnectorModule: \"com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule\"\n========================\nEnd of classname legend:\n========================\n\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProvisionException.toProvisionException(InternalProvisionException.java:251)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1104)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl.getInstance(InjectorImpl.java:1139)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelationInternal(CreatableRelationProviderHelper.java:114)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createBigQueryInsertableRelation(CreatableRelationProviderHelper.java:95)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:47)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: A project ID is required for this service but could not be determined from the builder or the environment.  Please set a project ID using the builder.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.common.base.Preconditions.checkArgument(Preconditions.java:143)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.ServiceOptions.<init>(ServiceOptions.java:304)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions.<init>(BigQueryOptions.java:92)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions.<init>(BigQueryOptions.java:30)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions$Builder.build(BigQueryOptions.java:87)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryOptions.getDefaultInstance(BigQueryOptions.java:162)\n\tat com.google.cloud.bigquery.connector.common.BigQueryConfigurationUtil.lambda$defaultBilledProject$1(BigQueryConfigurationUtil.java:41)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.common.base.Absent.or(Absent.java:61)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConfig.from(SparkBigQueryConfig.java:330)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConfig.from(SparkBigQueryConfig.java:247)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule.lambda$provideSparkBigQueryConfig$0(SparkBigQueryConnectorModule.java:80)\n\tat java.base/java.util.Optional.orElseGet(Optional.java:364)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule.provideSparkBigQueryConfig(SparkBigQueryConnectorModule.java:78)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule$$FastClassByGuice$$1232932.GUICE$TRAMPOLINE(<generated>)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryConnectorModule$$FastClassByGuice$$1232932.apply(<generated>)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod$FastClassProviderMethod.doProvision(ProviderMethod.java:260)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderMethod.doProvision(ProviderMethod.java:171)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.provision(InternalProviderInstanceBindingImpl.java:185)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalProviderInstanceBindingImpl$CyclicFactory.get(InternalProviderInstanceBindingImpl.java:162)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:40)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.SingletonScope$1.get(SingletonScope.java:169)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.inject.internal.InjectorImpl$1.get(InjectorImpl.java:1101)\n\t... 46 more\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '~/.config/gcloud/application_default_credentials.json'\n",
    "os.environ['GCLOUD_PROJECT'] = 'data-engineering-assignment-2'\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"tmp-bucket-for-data-engineering\"  # use your bucket \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "flight_counts.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.flight_counts') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n",
    "\n",
    "top_10_destinations.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.popular_dests') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n",
    "\n",
    "gate_table.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.gate_utilization') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n",
    "\n",
    "pier_table.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.pier_utilization') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n",
    "\n",
    "service_type_table.write.format('bigquery') \\\n",
    "  .option('table', 'schiphol_data.service_type_utilization') \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "22fe5b98-dba1-4854-adfc-9e89a4c9c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
