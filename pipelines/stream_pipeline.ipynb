{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.11/site-packages (3.13.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (1.59.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.14.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (1.22.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.6.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (23.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.61.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.24.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.59.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, concat, col, lit\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, DateType, IntegerType, FloatType, ArrayType, BooleanType\n",
    "from time import sleep\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"SparkStreamFlightData\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- lastUpdatedAt: string (nullable = true)\n",
      " |    |-- actualLandingTime: string (nullable = true)\n",
      " |    |-- actualOffBlockTime: string (nullable = true)\n",
      " |    |-- aircraftRegistration: string (nullable = true)\n",
      " |    |-- aircraftType: struct (nullable = true)\n",
      " |    |    |-- iataMain: string (nullable = true)\n",
      " |    |    |-- iataSub: string (nullable = true)\n",
      " |    |-- baggageClaim: struct (nullable = true)\n",
      " |    |    |-- belts: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- checkinAllocations: string (nullable = true)\n",
      " |    |-- codeshares: struct (nullable = true)\n",
      " |    |    |-- codeshares: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- estimatedLandingTime: string (nullable = true)\n",
      " |    |-- expectedTimeBoarding: string (nullable = true)\n",
      " |    |-- expectedTimeGateClosing: string (nullable = true)\n",
      " |    |-- expectedTimeGateOpen: string (nullable = true)\n",
      " |    |-- expectedTimeOnBelt: string (nullable = true)\n",
      " |    |-- expectedSecurityFilter: string (nullable = true)\n",
      " |    |-- flightDirection: string (nullable = true)\n",
      " |    |-- flightName: string (nullable = true)\n",
      " |    |-- flightNumber: integer (nullable = true)\n",
      " |    |-- gate: string (nullable = true)\n",
      " |    |-- pier: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- isOperationalFlight: boolean (nullable = true)\n",
      " |    |-- mainFlight: string (nullable = true)\n",
      " |    |-- prefixIATA: string (nullable = true)\n",
      " |    |-- prefixICAO: string (nullable = true)\n",
      " |    |-- airlineCode: integer (nullable = true)\n",
      " |    |-- publicEstimatedOffBlockTime: string (nullable = true)\n",
      " |    |-- publicFlightState: struct (nullable = true)\n",
      " |    |    |-- flightStates: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- route: struct (nullable = true)\n",
      " |    |    |-- destinations: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- eu: string (nullable = true)\n",
      " |    |    |-- visa: boolean (nullable = true)\n",
      " |    |-- scheduleDateTime: string (nullable = true)\n",
      " |    |-- scheduleDate: string (nullable = true)\n",
      " |    |-- scheduleTime: string (nullable = true)\n",
      " |    |-- serviceType: string (nullable = true)\n",
      " |    |-- terminal: integer (nullable = true)\n",
      " |    |-- transferPositions: string (nullable = true)\n",
      " |    |-- schemaVersion: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- estimatedLandingTime: string (nullable = true)\n",
      " |-- flightName: string (nullable = true)\n",
      " |-- scheduleDateTime: string (nullable = true)\n",
      " |-- actualLandingTime: string (nullable = true)\n",
      " |-- destinations: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flight_schema = StructType([\n",
    "    StructField(\"lastUpdatedAt\", StringType(), True),\n",
    "    StructField(\"actualLandingTime\", StringType(), True),\n",
    "    StructField(\"actualOffBlockTime\", StringType(), True),\n",
    "    StructField(\"aircraftRegistration\", StringType(), True),\n",
    "    StructField(\"aircraftType\", StructType([\n",
    "        StructField(\"iataMain\", StringType(), True),\n",
    "        StructField(\"iataSub\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"baggageClaim\", StructType([\n",
    "        StructField(\"belts\", ArrayType(StringType()), True)\n",
    "    ]), True),\n",
    "    StructField(\"checkinAllocations\", StringType(), True),\n",
    "    StructField(\"codeshares\", StructType([\n",
    "        StructField(\"codeshares\", ArrayType(StringType()), True)\n",
    "    ]), True),\n",
    "    StructField(\"estimatedLandingTime\", StringType(), True),\n",
    "    StructField(\"expectedTimeBoarding\", StringType(), True),\n",
    "    StructField(\"expectedTimeGateClosing\", StringType(), True),\n",
    "    StructField(\"expectedTimeGateOpen\", StringType(), True),\n",
    "    StructField(\"expectedTimeOnBelt\", StringType(), True),\n",
    "    StructField(\"expectedSecurityFilter\", StringType(), True),\n",
    "    StructField(\"flightDirection\", StringType(), True),\n",
    "    StructField(\"flightName\", StringType(), True),\n",
    "    StructField(\"flightNumber\", IntegerType(), True),\n",
    "    StructField(\"gate\", StringType(), True),\n",
    "    StructField(\"pier\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"isOperationalFlight\", BooleanType(), True),\n",
    "    StructField(\"mainFlight\", StringType(), True),\n",
    "    StructField(\"prefixIATA\", StringType(), True),\n",
    "    StructField(\"prefixICAO\", StringType(), True),\n",
    "    StructField(\"airlineCode\", IntegerType(), True),\n",
    "    StructField(\"publicEstimatedOffBlockTime\", StringType(), True),\n",
    "    StructField(\"publicFlightState\", StructType([\n",
    "        StructField(\"flightStates\", ArrayType(StringType()), True)\n",
    "    ]), True),\n",
    "    StructField(\"route\", StructType([\n",
    "        StructField(\"destinations\", ArrayType(StringType()), True),\n",
    "        StructField(\"eu\", StringType(), True),\n",
    "        StructField(\"visa\", BooleanType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"scheduleDateTime\", StringType(), True),\n",
    "    StructField(\"scheduleDate\", StringType(), True),\n",
    "    StructField(\"scheduleTime\", StringType(), True),\n",
    "    StructField(\"serviceType\", StringType(), True),\n",
    "    StructField(\"terminal\", IntegerType(), True),\n",
    "    StructField(\"transferPositions\", StringType(), True),\n",
    "    StructField(\"schemaVersion\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the whole dataset as a batch\n",
    "kafkaStream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"subscribe\", \"flight\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# input_df.printSchema()\n",
    "# Transform to Output DataFrame\n",
    "value_df = kafkaStream.select(from_json(col(\"value\").cast(\"string\"),flight_schema).alias(\"value\"))\n",
    "\n",
    "value_df.printSchema()\n",
    "\n",
    "exploded_df = value_df.selectExpr('value.estimatedLandingTime', 'value.flightName', 'value.scheduleDateTime', 'value.actualLandingTime',\n",
    "                                  'explode(value.route.destinations) as destinations')\n",
    "\n",
    "exploded_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "query = exploded_df \\\n",
    "    .select(col(\"destinations\").alias(\"value\")) \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/checkpoints/results\") \\\n",
    "    .option(\"topic\", \"results\") \\\n",
    "    .start()\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"tmp-bucket-for-data-engineering\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_foreach_batch_function(df, batch_id):\n",
    "   # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'schiphol_data.flights') \\\n",
    "      .mode(\"append\") \\\n",
    "      .save()\n",
    "\n",
    "# Write to a sink - here, the output is written to a Big Query Table\n",
    "# ProcessingTime trigger with 60-seconds micro-batch interval as the dataset is large and does not get updated within the 60 second timeframe\n",
    "# Using output mode append as only new rows need to be appeneded to BigQuery and no aggregating is done with previous data\n",
    "flightsQuery = exploded_df.writeStream.outputMode(\"append\") \\\n",
    "                        .trigger(processingTime = '60 seconds').foreachBatch(my_foreach_batch_function).start()\n",
    "try:\n",
    "    flightsQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    flightsQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
